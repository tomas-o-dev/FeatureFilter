{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "> **Essential ML process for Intrusion Detection**\n",
    "<br>` python  3.7.13    scikit-learn  1.0.2 `\n",
    "<br>`numpy   1.19.5          pandas  1.3.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_S34U5S-i69d"
   },
   "source": [
    "**Import the main libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "\n",
    "import os\n",
    "data_path = '../datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9IZetEZ8jQJm"
   },
   "source": [
    "**Import the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using boosted Train and preprocessed Test\n",
    "\n",
    "data_file = os.path.join(data_path, 'NSL_ppTrain.csv') \n",
    "train_df = pandas.read_csv(data_file)\n",
    "print('Train Dataset: {} rows, {} columns'.format(train_df.shape[0], train_df.shape[1]))\n",
    "\n",
    "data_file = os.path.join(data_path, 'NSL_ppTest.csv') \n",
    "test_df = pandas.read_csv(data_file)\n",
    "print('Test Dataset: {} rows, {} columns'.format(test_df.shape[0], test_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**Data Preparation and EDA** (consistency checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* _Check column names of numeric attributes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trnn = train_df.select_dtypes(include=['float64','int64']).columns\n",
    "tstn = test_df.select_dtypes(include=['float64','int64']).columns\n",
    "trndif = numpy.setdiff1d(trnn, tstn)\n",
    "tstdif = numpy.setdiff1d(tstn, trnn)\n",
    "\n",
    "print(\"Numeric features in the train_set that are not in the test_set: \",end='')\n",
    "if len(trndif) > 0:\n",
    "    print('\\n',trndif)\n",
    "else:\n",
    "    print('None')\n",
    "\n",
    "print(\"Numeric features in the test_set that are not in the train_set: \",end='')\n",
    "if len(tstdif) > 0:\n",
    "    print('\\n',tstdif)\n",
    "else:\n",
    "    print('None')\n",
    "\n",
    "print()\n",
    "# correct any differences here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Check column names of categorical attributes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trnn = train_df.select_dtypes(include=['object']).columns\n",
    "tstn = test_df.select_dtypes(include=['object']).columns\n",
    "trndif = numpy.setdiff1d(trnn, tstn)\n",
    "tstdif = numpy.setdiff1d(tstn, trnn)\n",
    "\n",
    "print(\"Categorical features in the train_set that are not in the test_set: \",end='')\n",
    "if len(trndif) > 0:\n",
    "    print('\\n',trndif)\n",
    "else:\n",
    "    print('None')\n",
    "\n",
    "print(\"Categorical features in the test_set that are not in the train_set: \",end='')\n",
    "if len(tstdif) > 0:\n",
    "    print('\\n\\t',tstdif)\n",
    "else:\n",
    "    print('None')\n",
    "\n",
    "print()\n",
    "# correct any differences here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Check categorical feature values:<br>\n",
    "differences will be resolved by one-hot encoding the combined test and train sets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trnn = train_df.select_dtypes(include=['object']).columns\n",
    "for col in trnn:\n",
    "    tr = train_df[col].unique()\n",
    "    ts = test_df[col].unique()\n",
    "    trd = numpy.setdiff1d(tr, ts)\n",
    "    tsd = numpy.setdiff1d(ts, tr)\n",
    "    \n",
    "    print(col,'::> ')\n",
    "    print(\"\\tUnique text values in the train_set that are not in the test_set: \",end='')\n",
    "    if len(trd) > 0:\n",
    "        print('\\n\\t',trd)\n",
    "    else:\n",
    "        print('None')\n",
    "    \n",
    "    print(\"\\tUnique text values in the test_set that are not in the train_set: \",end='')\n",
    "    if len(tsd) > 0:\n",
    "        print('\\n\\t',tsd)\n",
    "    else:\n",
    "        print('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Combine for processing classification target and text features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pandas.concat([train_df, test_df])\n",
    "print('Combined Dataset: {} rows, {} columns'.format(\n",
    "    combined_df.shape[0], combined_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classification Target feature:\n",
    "# two columns of labels are available \n",
    "#    * Two-class: labels     * Multiclass: atakcat\n",
    "\n",
    "# Two-class: Reduce the detailed attack labels to 'normal' or 'attack'\n",
    "labels_df = combined_df['label'].copy()\n",
    "labels_df[labels_df != 'normal'] = 'attack'\n",
    "\n",
    "# drop target features \n",
    "combined_df.drop(['label'], axis=1, inplace=True)\n",
    "combined_df.drop(['atakcat'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# one-Hot encoding the remaining text features\n",
    "categori = combined_df.select_dtypes(include=['object']).columns\n",
    "category_cols = categori.tolist()\n",
    "\n",
    "features_df = pandas.get_dummies(combined_df, columns=category_cols)\n",
    "features_df.info(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restore the train // test split: slice 1 Dataframe into 2 \n",
    "# pandas has a lot of rules about returning a 'view' vs. a copy from slice\n",
    "# so we force it to create a new dataframe [avoiding SettingWithCopy Warning]\n",
    "features_train = features_df.iloc[:len(train_df),:].copy()    # X_train\n",
    "features_test = features_df.iloc[len(train_df):,:].copy()     # X_test\n",
    "\n",
    "# Restore the train // test split: slice 1 Series into 2 \n",
    "labels_train = labels_df[:len(train_df)]               # y_train\n",
    "labels_test = labels_df[len(train_df):]                # y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**QLCFF: Quick Layered Correlation-based Feature Filter**<br>\n",
    "> **_library requirements:_**<br>\n",
    "    * Dataframe of features (text values may be one-hot encoded)<br>\n",
    "    * Class labels in np.ndarray or pd.Series with shape (n,1)<br>\n",
    "    * Binary classification (not multiclass or multilabel)<br><br>\n",
    "> **_workflow:_**<br>\n",
    "    1. Instantiate a discretizer<br>\n",
    "    2. get the binned dataframe from the discretizer<br>\n",
    "    3. Apply filters to the binned dataset<br>\n",
    "    4. Apply drop (or keep) lists to the real features dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "_**import the local library**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent folder path where lib folder is\n",
    "import sys\n",
    "if \"..\" not in sys.path:import sys; sys.path.insert(0, '..') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QLCFF import unifhgm, MDLP, ChiMerge\n",
    "# three distinct discretizers can be instantiated\n",
    "\n",
    "from QLCFF import filter_fcy, filter_fdr, filter_fcc\n",
    "# filter_fcy: floor filter, feature-to-label (f2y) correlations  \n",
    "#       filter all with low correlation to target\n",
    "# filter_fdr: sklearn univariate chi-square test: FDR or FWE\n",
    "#       https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
    "# filter_fcc: FCBF-style, filter on feature-to-feature (f2f) correlations\n",
    "#       using Pearson correlation (PC) or symmetric uncertainty (SU)\n",
    "\n",
    "from QLCFF import get_filter, rpt_ycor, rpt_fcor\n",
    "# get_filter: returns list of features from f2y report\n",
    "# rpt_ycor: print feature-to-label (f2y) correlations \n",
    "# rpt_fcor: print feature-to-feature (f2f) correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "_new in v0.05_\n",
    ">   1. Instantiate a discretizer<br>\n",
    "2. get the binned dataframe from the discretizer<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three distinct discretizers can be instantiated:\n",
    "#    unifhgm: uniform (np.linspace()) or histogram\n",
    "#        Optional : mkbins in ['ten','sqrt','log','hgrm']\n",
    "#        default is hgrm: applies np.histogram(feature, bins='auto')\n",
    "#    MDLP algorithm  [1]\n",
    "#        Optional : mkbins in ['ten','sqrt','log']\n",
    "#                   joblib processes, verbose level \n",
    "#                          defaults: numjobs=1, msglvl=0 \n",
    "#    ChiMerge algorithm  [2]\n",
    "#        Optional : mkbins in ['ten','sqrt','log']\n",
    "#                   joblib processes, verbose level \n",
    "#                          defaults: numjobs=1, msglvl=0 \n",
    "#    ten:  number of bins is always ten - default for MDLP and ChiMerge [3,4]\n",
    "#    sqrt: number of bins is sqrt(len(np.unique(feature)))   [5]\n",
    "#    log:  number of bins is log10(len(np.unique(feature)))  [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>_**binning: uniform or histogram**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: instantiate, then call fit() or fit_transform()\n",
    "hgmb = unifhgm(mkbins='hgrm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit() calls the preprocessor and the discretizer\n",
    "# Requires:\n",
    "#    features as pd.dataframe, labels as array-like\n",
    "# Optional:\n",
    "#    print detailed report (boolean) - default False\n",
    "# preprocessor:\n",
    "#    1. selects only column dtypes np.number and pd or np boolean\n",
    "#    2. nomalizes all columns with signed dtypes to positive numbers\n",
    "#    3. nomalizes all columns with boolean dtypes to zero//one\n",
    "#    text labels are converted with sklearn LabelEncoder()\n",
    "\n",
    "hgmb.fit(features_test, labels_test,\n",
    "         detail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binned bataframe is an attribute\n",
    "#     transform() is just a getter method, with optional detail\n",
    "\n",
    "X_hgmbinz = hgmb.transform(features_test, detail=True)\n",
    "#X_hgmbinz = hgmb.binned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hgmbinz.info(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed list of bin edges is an attribute\n",
    "# hgmb.cutpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>_**binning: MDLP algorithm**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: instantiate, then call fit() or fit_transform()\n",
    "mdlpb = MDLP(mkbins='log',\n",
    "            numjobs= -2, msglvl=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_mdlbinz = mdlpb.fit_transform(features_test, labels_test,\n",
    "                                detail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mdlbinz.info(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>_**binning: ChiMerge algorithm**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: instantiate, then call fit() or fit_transform()\n",
    "chi_merge = ChiMerge(mkbins='sqrt',\n",
    "                    numjobs= -2, msglvl=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_chibinz = chi_merge.fit_transform(features_test, labels_test, \n",
    "                                    detail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_chibinz.info(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "_same as v0.04, with minor changes to signatures_\n",
    ">   3. Apply filters to the binned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_fcy: floor filter, feature-to-label (f2y) correlations  \n",
    "#       filter all with low correlation to target\n",
    "# filter_fdr: sklearn univariate chi-square test: FDR or FWE\n",
    "#       https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
    "\n",
    "# floor filter and univariate tests both filter features\n",
    "#    on the basis that low correlation with the target labels\n",
    "#    means low utility for distinguishing class membership\n",
    "\n",
    "# chi_sq is a formal test for independence, fwe will select more to drop than fdr,\n",
    "#    standard thresholds are 0.1, 0.05, 0.01; lower will select more to drop \n",
    "# floor filter will select all from either univariate test, and more\n",
    "\n",
    "# one of these can be applied before\n",
    "#     filter_fcc: FCBF-style, filter on feature-to-feature (f2f) correlations\n",
    "#           using Pearson correlation (PC) or symmetric uncertainty (SU)\n",
    "# to create layered feature selection filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters still require numeric targets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "ynum = LabelEncoder().fit_transform(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe to use for filters\n",
    "binned_df = X_hgmbinz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>_**filter_fcy(): floor filter**_ - as single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_fcy: floor filter, feature-to-label (f2y) correlations  \n",
    "#       drop if f2y_pc < minpc or f2y_su < minsu\n",
    "\n",
    "# Requires: binned_df, numeric_labels\n",
    "# Optional:\n",
    "#    minpc : threshold for pearson correlation    default=0.1\n",
    "#    minsu : threshold for symmetric uncertainty  default=0.01 \n",
    "# Returns: f2y report for features to drop\n",
    "#          f2y report for features to keep\n",
    "\n",
    "print('\\nFloor filter (low correlation with target)')\n",
    "nfdrop_yc, nfkeep_yc = filter_fcy(binned_df, ynum)\n",
    "\n",
    "if (len(nfdrop_yc) > 1):\n",
    "    nfdrop = get_filter(nfdrop_yc)\n",
    "    nfkeep = get_filter(nfkeep_yc)\n",
    "    \n",
    "    print('To Keep:',len(nfkeep),'features')\n",
    "#    rpt_ycor(nfkeep_yc)\n",
    "    print('To Drop:',len(nfdrop),'features')\n",
    "    rpt_ycor(nfdrop_yc)\n",
    "else:\n",
    "    print('No features were selected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_same as v0.04_\n",
    ">   4. Apply drop (or keep) list to the real features dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the \"keep\" filter to the real dataset\n",
    "# pandas has a lot of rules about returning a 'view' vs. a copy\n",
    "#        so we force it to create a new dataframe \n",
    "# python assigns by reference (namespaces) so keeping a reference\n",
    "#        to the original is minimal overhead\n",
    "\n",
    "# features_df_original = features_df\n",
    "filtered_df = features_df[nfkeep].copy()\n",
    "filtered_df.info(verbose=False)\n",
    "# features_df = filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<br>_**FDR + SU + Pearson**_ - multiple layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**filter_fdr(): univariate chi_sq layer**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_fdr: sklearn univariate chi-square test: FDR or FWE\n",
    "#    fwe will select more to drop than fdr,\n",
    "#    standard thresholds are 0.1, 0.05, 0.01; lower will select more to drop\n",
    "\n",
    "# Requires: binned_df, numeric_labels\n",
    "# Optional:\n",
    "#    plvl : threshold (alpha) for chi_sq test  default=0.5\n",
    "#    usefdr : boolean, fdr if True, else fwe   default=True\n",
    "# Returns: f2y report for features to drop\n",
    "\n",
    "# set test:\n",
    "# tst = 'FDR'\n",
    "# ufd = True\n",
    "# = or =\n",
    "tst = 'FWE'\n",
    "ufd = False\n",
    "\n",
    "print('\\nUnivariate chi-sq',tst,'test')\n",
    "uvdrop_yc = filter_fdr(binned_df, ynum, usefdr=ufd, plvl=0.01)\n",
    "\n",
    "if (len(uvdrop_yc) > 1):\n",
    "    uvdrop = get_filter(uvdrop_yc)\n",
    "\n",
    "    print('Progressive_filtering: Dropping',len(uvdrop),'features')\n",
    "    binned_df.drop(uvdrop, axis = 1, inplace = True)\n",
    "\n",
    "    print('\\nFiltered:',tst,'Layer')\n",
    "    rpt_ycor(uvdrop_yc)\n",
    "else:\n",
    "    uvdrop = uvdrop_yc \n",
    "    print('\\n',tst,'Layer: No features were selected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    " ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCBF-Pearson and FCBF-SU Layers use the same code:  filter_fcc()\n",
    "#            metric depends on the boolean argument:  usesu\n",
    "# just use appropriate names for the return values\n",
    "#     features to keep are called \"predominant features\" in the FCBF paper [6]; \n",
    "#     they act as proxies for the highly correlated features to drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>_**filter_fcc(): FCBF-SU layer**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_fcc: FCBF-style, filter on feature-to-feature (f2f) correlations\n",
    "\n",
    "# Requires: binned_df, numeric_labels\n",
    "# Optional:\n",
    "#    hipc : threshold for \"high\" f2f pearson correlation  default=0.7\n",
    "#    hisu : threshold for \"high\" f2f su correlation       default=0.7\n",
    "#    usesu : boolean, use su as metric if True, else pearson\n",
    "# Returns: f2y report for features to drop\n",
    "#          f2y report for features to keep\n",
    "#          f2f above threshold report \n",
    "\n",
    "sudrop_yc, sukeep_yc, su_hicorr = filter_fcc(binned_df, ynum, usesu=True)\n",
    "\n",
    "if (len(sudrop_yc) > 1):\n",
    "    sudrop = get_filter(sudrop_yc)\n",
    "\n",
    "    print('Progressive_filtering: Dropping',len(sudrop),'features')\n",
    "    binned_df.drop(sudrop, axis = 1, inplace = True)\n",
    "\n",
    "    print('\\nKept: FCBF (SU) Layer')\n",
    "    rpt_ycor(sukeep_yc)\n",
    "    print('\\nFiltered: FCBF (SU) Layer')\n",
    "    rpt_ycor(sudrop_yc)\n",
    "    print('\\nHighly correlated features: FCBF (SU) Layer')\n",
    "    rpt_fcor(su_hicorr)\n",
    "else:\n",
    "    sudrop = sudrop_yc \n",
    "    print('\\nFCBF (SU) Layer\\nNo features were selected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>_**filter_fcc(): FCBF-Pearson layer**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_fcc: FCBF-style, filter on feature-to-feature (f2f) correlations\n",
    "\n",
    "# Requires: binned_df, numeric_labels\n",
    "# Optional:\n",
    "#    hipc : threshold for \"high\" f2f pearson correlation  default=0.7\n",
    "#    hisu : threshold for \"high\" f2f su correlation       default=0.7\n",
    "#    usesu : boolean, use su as metric if True, else pearson\n",
    "# Returns: f2y report for features to drop\n",
    "#          f2y report for features to keep\n",
    "#          f2f above threshold report\n",
    "\n",
    "pcdrop_yc, pckeep_yc, pc_hicorr = filter_fcc(binned_df, ynum)\n",
    "\n",
    "if (len(pcdrop_yc) > 1):\n",
    "    pcdrop = get_filter(pcdrop_yc)\n",
    "\n",
    "#    print('Progressive_filtering: Dropping',len(pcdrop),'features')\n",
    "#    binned_df.drop(pcdrop, axis = 1, inplace = True)\n",
    "\n",
    "    print('\\nKept: Pearson Layer')\n",
    "    rpt_ycor(pckeep_yc)\n",
    "    print('\\nFiltered: Pearson Layer')\n",
    "    rpt_ycor(pcdrop_yc)\n",
    "    print('\\nHighly correlated features: Pearson Layer')\n",
    "    rpt_fcor(pc_hicorr)\n",
    "else:\n",
    "    pcdrop = pcdrop_yc \n",
    "    print('\\nPearson Layer\\nNo features were selected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<i>same as v0.04</i>\n",
    ">   4. Apply drop lists to the real features dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## QLCFF: Quick Layered Correlation-based Feature Filter\n",
    "## full layered drop filter for real DF \n",
    "QLCFFilter = []\n",
    "QLCFFilter.extend(x for x in uvdrop if x not in QLCFFilter)\n",
    "QLCFFilter.extend(x for x in pcdrop if x not in QLCFFilter)\n",
    "QLCFFilter.extend(x for x in sudrop if x not in QLCFFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features_df_original = features_df\n",
    "filtered_df = features_df.drop(QLCFFilter, axis = 1)\n",
    "filtered_df.info(verbose=False)\n",
    "# features_df = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    " ***"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[1] Fayyad, U. M., and Irani, K. B. (1993). \"Multiinterval discretization of \n",
    "    continuous-valued attributes for classifcation learning\", Proc. 13th \n",
    "    Int. Joint Conference on Artifcial Intelligence, pp. 1022-1027\n",
    "[2] Kerber R. (1992). \"Chimerge: Discretization of numeric attributes\", \n",
    "    Proc. 10th National Conference on Artifcial Intelligence (AAAI'92), pp. 123–128\n",
    "[3] Dougherty J., Kohavi, R., and Sahami, M. (1995), “Supervised and unsupervised\n",
    "    discretization of continuous features”, Proc. ICML 1995, pp. 194–202\n",
    "[4] Yang, Y. and Webb, G. I. (2002), “A comparative study of discretization methods \n",
    "    for naive-bayes classifiers”, Proc. PKAW 2002, pp. 159-173\n",
    "[5] Yang, Y. and Webb, G. I. (2001), “Proportional k-interval discretization \n",
    "    for naive-bayes classifiers”, in Machine learning: ECML 2001, pp. 564–575\n",
    "[6] Lei Yu and Huan Liu (2003), \"Feature Selection for High-Dimensional Data: \n",
    "    A Fast Correlation-Based Filter Solution\", Proc. 20th ICML 2003, pp. 856-863"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
